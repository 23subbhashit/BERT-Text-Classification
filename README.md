# BERT-Text-Classification


BERT is a method of pre-training language representations, meaning training of a general-purpose "language understanding" model on a large text corpus (like Wikipedia), and then using that model for downstream NLP tasks (like question answering). BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP.

Here, I have implemented it to perform **Covid Tweet Classification**.

Use GPU while using BERT as it may take hours to train using CPU.
